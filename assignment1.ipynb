{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae597d45",
   "metadata": {},
   "source": [
    "# base_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7882a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.word_frequency import WordFrequency\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Base class for dictionary implementations. DON'T CHANGE THIS FILE.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "class BaseDictionary:\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        @return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def autocomplete(self, prefix_word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'prefix_word' as a prefix\n",
    "        @param prefix_word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'prefix_word'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e3dff",
   "metadata": {},
   "source": [
    "# list_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e00c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.word_frequency import WordFrequency\n",
    "from dictionary.base_dictionary import BaseDictionary\n",
    "import time\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required TO BE IMPLEMENTED. List-based dictionary implementation.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class ListDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.data: list = None;\n",
    "\n",
    "    def partition(self, data, i, k, by):\n",
    "        midpoint = i + (k - i) // 2\n",
    "        pivot = getattr(data[midpoint], by)\n",
    "\n",
    "        done = False\n",
    "        l = i\n",
    "        h = k\n",
    "        while not done:\n",
    "            while getattr(data[l], by) < pivot:\n",
    "                l = l + 1\n",
    "            while pivot < getattr(data[h], by):\n",
    "                h = h - 1\n",
    "            if l >= h:\n",
    "                done = True\n",
    "            else:\n",
    "                temp = data[l]\n",
    "                data[l] = data[h]\n",
    "                data[h] = temp\n",
    "                l = l + 1\n",
    "                h = h - 1\n",
    "        return h\n",
    "\n",
    "    def quicksort(self, data, i, k, by: str):\n",
    "        j = 0\n",
    "        if i >= k:\n",
    "            return\n",
    "        j = self.partition(data, i, k, by)\n",
    "        self.quicksort(data, i, j, by)\n",
    "        self.quicksort(data, j + 1, k, by)\n",
    "        return\n",
    "\n",
    "    # merge sort algorithm sourced from Zybooks\n",
    "    def merge(self, data, i, j, k, by: str):\n",
    "        merged_size = k - i + 1  # Size of merged partition\n",
    "        merged_data = [0] * merged_size  # Dynamically allocates temporary array\n",
    "        # for merged numbers\n",
    "        merge_pos = 0  # Position to insert merged number\n",
    "        left_pos = i  # Initialize left partition position\n",
    "        right_pos = j + 1  # Initialize right partition position\n",
    "\n",
    "        # Add smallest element from left or right partition to merged numbers\n",
    "        while left_pos <= j and right_pos <= k:\n",
    "            if getattr(data[left_pos], by) <= getattr(data[right_pos], by):\n",
    "                merged_data[merge_pos] = data[left_pos]\n",
    "                left_pos += 1\n",
    "            else:\n",
    "                merged_data[merge_pos] = data[right_pos]\n",
    "                right_pos += 1\n",
    "            merge_pos = merge_pos + 1\n",
    "\n",
    "        # If left partition is not empty, add remaining elements to merged numbers\n",
    "        while left_pos <= j:\n",
    "            merged_data[merge_pos] = data[left_pos]\n",
    "            left_pos += 1\n",
    "            merge_pos += 1\n",
    "\n",
    "        # If right partition is not empty, add remaining elements to merged numbers\n",
    "        while right_pos <= k:\n",
    "            merged_data[merge_pos] = data[right_pos]\n",
    "            right_pos = right_pos + 1\n",
    "            merge_pos = merge_pos + 1\n",
    "\n",
    "        # Copy merge number back to numbers\n",
    "        for merge_pos in range(merged_size):\n",
    "            data[i + merge_pos] = merged_data[merge_pos]\n",
    "\n",
    "    def merge_sort(self, data, i, k, by: str):\n",
    "        j = 0\n",
    "\n",
    "        if i < k:\n",
    "            j = (i + k) // 2  # Find the midpoint in the partition\n",
    "\n",
    "            # Recursively sort left and right partitions\n",
    "            self.merge_sort(data, i, j, by)\n",
    "            self.merge_sort(data, j + 1, k, by)\n",
    "\n",
    "            # Merge left and right partition in sorted order\n",
    "            self.merge(data, i, j, k, by)\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"\"\n",
    "        for items in self.data:\n",
    "            str += f\"({items.word}, {items.frequency})\\n\"\n",
    "        return str\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        self.data = words_frequencies\n",
    "        # self.quicksort(self.data, 0, len(self.data) - 1, \"word\")\n",
    "        self.data.sort(key=lambda x: x.word)\n",
    "\n",
    "    def binSearch(self, word:str) -> (bool, int):\n",
    "        \"\"\"\n",
    "        binary search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: (True, the index of word_frequencies) OR (False, the index of word_frequencies to be inserted into)\n",
    "        \"\"\"\n",
    "        low, mid, high = 0, 0, len(self.data) - 1\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            if self.data[mid].word > word:\n",
    "                high = mid - 1\n",
    "            elif self.data[mid].word < word:\n",
    "                low = mid + 1\n",
    "            else:\n",
    "                return (True, mid)\n",
    "        if word >= self.data[mid].word:\n",
    "            return (False, mid + 1)\n",
    "        else:\n",
    "            return (False, mid)\n",
    "\n",
    "    def binSearchAC(self, prefix_word:str) -> int:\n",
    "        \"\"\"\n",
    "        binary search for a prefix\n",
    "        @param prefix: the prefix to be searched\n",
    "        @return: if found: the index of the first encountered word with the same prefix; if not: -1\n",
    "        \"\"\"\n",
    "        # The implementation is almost identical to binSearch except that prefix is compared to word upto its own length\n",
    "        low, mid, high = 0, 0, len(self.data) - 1\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            if self.data[mid].word[:len(prefix_word)] > prefix_word:\n",
    "                high = mid - 1\n",
    "            elif self.data[mid].word[:len(prefix_word)] < prefix_word:\n",
    "                low = mid + 1\n",
    "            else:\n",
    "                return mid\n",
    "        return -1\n",
    "\n",
    "    def getAutocompleteList(self, prefix_word: str, idx: int) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        add all the words sharing the same prefix_word to a list and return it unsorted\n",
    "        @param prefix_word: the prefix_word to be searched, idx: the starting index to search from in both directions (left and right)\n",
    "        @return: an unsorted list containing all the words sharing the same prefix_word\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        # Add the first word\n",
    "        res.append(self.data[idx])\n",
    "        left_idx = idx - 1\n",
    "        right_idx = idx + 1\n",
    "\n",
    "        if left_idx >= 0:\n",
    "            curr_left_word = self.data[left_idx].word[:len(prefix_word)]\n",
    "        # Add words to the left of the first word\n",
    "        while left_idx >= 0 and curr_left_word == prefix_word:\n",
    "            res.append(self.data[left_idx])\n",
    "            left_idx -= 1\n",
    "            curr_left_word = self.data[left_idx].word[:len(prefix_word)]\n",
    "\n",
    "        if right_idx <= len(self.data) - 1:\n",
    "            curr_right_word = self.data[right_idx].word[:len(prefix_word)]\n",
    "        # Add words to the right of the first word\n",
    "        while right_idx <= len(self.data) - 1 and curr_right_word == prefix_word:\n",
    "            res.append(self.data[right_idx])\n",
    "            right_idx += 1\n",
    "            curr_right_word = self.data[right_idx].word[:len(prefix_word)]\n",
    "\n",
    "        return res\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        # Employ binary search\n",
    "        isFound, foundIdx = self.binSearch(word)\n",
    "        if not isFound:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.data[foundIdx].frequency\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        # Employ binary search\n",
    "        word = word_frequency.word\n",
    "        isFound, foundIdx = self.binSearch(word)\n",
    "        actualLength = len(self.data)\n",
    "        if isFound:\n",
    "            return False\n",
    "        # If not found, add the word in self.data\n",
    "        else:\n",
    "            # Create space to shuffle elements to the right by 1\n",
    "            self.data.append(None)\n",
    "            for i in range(actualLength - 1, foundIdx - 1, -1):\n",
    "                self.data[i + 1] = self.data[i]\n",
    "            self.data[foundIdx] = word_frequency\n",
    "            return True\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        isFound, foundIdx = self.binSearch(word)\n",
    "        if isFound:\n",
    "            for i in range(foundIdx, len(self.data) - 1):\n",
    "                self.data[i] = self.data[i + 1]\n",
    "            # In all cases, the last element will be deleted if the word is found\n",
    "            del self.data[-1]\n",
    "            return True\n",
    "        # If found, delete the word in self.data\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    def autocomplete(self, prefix_word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'prefix_word' as a prefix\n",
    "        @param prefix_word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'prefix_word'\n",
    "        \"\"\"\n",
    "        # As soon as prefix_word matches with any word, scan all the words to its left and right and put them in a new list\n",
    "        # Iterate them only once to find the 3 most-frequent words\n",
    "        idx = self.binSearchAC(prefix_word)\n",
    "        if idx == -1:\n",
    "            return []\n",
    "        else:\n",
    "            lst = self.getAutocompleteList(prefix_word, idx)\n",
    "            self.merge_sort(lst, 0, len(lst) - 1, \"frequency\")\n",
    "            return lst[-3:][::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c970e52",
   "metadata": {},
   "source": [
    "# hashtable_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde39dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.base_dictionary import BaseDictionary\n",
    "from dictionary.word_frequency import WordFrequency\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required TO BE IMPLEMENTED. Hash-table-based dictionary.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class HashTableDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        self.data = {entry.word: entry.frequency for entry in words_frequencies}\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        return self.data.get(word, 0)\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        freq = self.search(word_frequency.word)\n",
    "        if freq > 0:\n",
    "            return False\n",
    "        else:\n",
    "            self.data[word_frequency.word] = word_frequency.frequency\n",
    "            return True\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        freq = self.search(word)\n",
    "        if freq > 0:\n",
    "            del self.data[word]\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def autocomplete(self, word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'word' as a prefix\n",
    "        @param word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'word'\n",
    "        \"\"\"\n",
    "        # Find the keys that start with a given prefix\n",
    "        autoDic = {key: freq for key, freq in self.data.items() if key.startswith(word)}\n",
    "        # Use Python's built-in sorting algorithm to sort autoDic by frequency and return the last three elements in descending order.\n",
    "        return [WordFrequency(key, freq) for key, freq in sorted(autoDic.items(), key=lambda item: item[1])][-3:][::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17642527",
   "metadata": {},
   "source": [
    "# tearnarysearchtree_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be71dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.base_dictionary import BaseDictionary\n",
    "from dictionary.word_frequency import WordFrequency\n",
    "from dictionary.node import Node\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required to be implemented. Ternary Search Tree implementation.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class TernarySearchTreeDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        for idx, entry in enumerate(words_frequencies):\n",
    "            self.add_word_frequency(entry)\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        endNode = self.search_from_node(self.root, word, 0)\n",
    "        if endNode == None:\n",
    "            return 0\n",
    "        elif endNode.end_word == False:\n",
    "            return 0\n",
    "        else:\n",
    "            return endNode.frequency\n",
    "\n",
    "\n",
    "\n",
    "    def search_from_node(self, currNode, word, currIdx):\n",
    "        \"\"\"\n",
    "        search for a word recursively\n",
    "        @param node, word, currIdx: node to start from, the word to be searched, currIdx to search curLetter at\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        if currNode == None or currNode.letter == None:\n",
    "            return None\n",
    "        if currNode.letter < currLetter:\n",
    "            return self.search_from_node(currNode.right, word, currIdx)\n",
    "        elif currNode.letter > currLetter:\n",
    "            return self.search_from_node(currNode.left, word, currIdx)\n",
    "        elif currIdx < len(word) - 1:\n",
    "            return self.search_from_node(currNode.middle, word, currIdx + 1)\n",
    "        else:\n",
    "            return currNode\n",
    "\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        return self.add_word_from_root(self.root, word_frequency.word, word_frequency.frequency, 0)\n",
    "\n",
    "    def add_word_from_root(self, currNode, word, freq, currIdx) -> bool:\n",
    "        \"\"\"\n",
    "        add a word recursively\n",
    "        @param currNode, word, freq, currIdx: currNode initially self.root; currIdx is used for the base case.\n",
    "        :return: True if addition is successful, false if the word being added already exists.\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        # Base case on the last word\n",
    "        if currIdx == len(word) - 1:\n",
    "            if currNode.letter == None:\n",
    "                currNode.letter = currLetter\n",
    "                currNode.frequency = freq\n",
    "                currNode.end_word = True\n",
    "                return True\n",
    "            # If currNode is the same as curLetter\n",
    "            else:\n",
    "                if currNode.letter > currLetter:\n",
    "                    if currNode.left == None:\n",
    "                        currNode.left = Node()\n",
    "                    currNode = currNode.left\n",
    "                    return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "                elif currNode.letter < currLetter:\n",
    "                    if currNode.right == None:\n",
    "                        currNode.right = Node()\n",
    "                    currNode = currNode.right\n",
    "                    return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "                else:\n",
    "                    if currNode.end_word == True:\n",
    "                        return False\n",
    "                    else:\n",
    "                        currNode.frequency = freq\n",
    "                        currNode.end_word = True\n",
    "                        return True\n",
    "        # Recursive case\n",
    "        else:\n",
    "            # When no word is present\n",
    "            if currNode.letter == None:\n",
    "                currNode.letter = currLetter\n",
    "                currNode.middle = Node()\n",
    "                currNode = currNode.middle\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx + 1)\n",
    "            elif currNode.letter < currLetter:\n",
    "                if currNode.right == None:\n",
    "                    currNode.right = Node()\n",
    "                currNode = currNode.right\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "            elif currNode.letter > currLetter:\n",
    "                if currNode.left == None:\n",
    "                    currNode.left = Node()\n",
    "                currNode = currNode.left\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "            else:\n",
    "                if currNode.middle == None:\n",
    "                    currNode.middle = Node()\n",
    "                currNode = currNode.middle\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx + 1)\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        # First, search for a word\n",
    "        # endNode = self.search_from_node(self.root, word, 0)\n",
    "        # if not endNode or endNode.end_word == False:\n",
    "        #     return False\n",
    "        # else:\n",
    "        #     endNode.frequency = None\n",
    "        #     endNode.end_word = False\n",
    "        #     if endNode.left == None and endNode.middle == None and endNode.right == None:\n",
    "        #         del endNode\n",
    "        # return True\n",
    "        deleteStatus = [False]\n",
    "        self.delete_from_node(self.root, word, 0, deleteStatus)\n",
    "        return deleteStatus[0]\n",
    "\n",
    "    def delete_from_node(self, currNode, word, currIdx, deleteStatus):\n",
    "        \"\"\"\n",
    "        delete a word recursively\n",
    "        @param prevNode, currNode, word, currIdx\n",
    "        @return: False if not found or end_word equals False, True if found and end_word equals True\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        if currNode == None or currNode.letter == None:\n",
    "            return False\n",
    "        if currNode.letter < currLetter:\n",
    "            if self.delete_from_node(currNode.right, word, currIdx, deleteStatus):\n",
    "                currNode.right = None\n",
    "            else:\n",
    "                return False\n",
    "        elif currNode.letter > currLetter:\n",
    "            if self.delete_from_node(currNode.left, word, currIdx, deleteStatus):\n",
    "                currNode.left = None\n",
    "            else:\n",
    "                return False\n",
    "        elif currIdx < len(word) - 1:\n",
    "            if self.delete_from_node(currNode.middle, word, currIdx + 1, deleteStatus):\n",
    "                currNode.middle = None\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if currNode.end_word:\n",
    "                deleteStatus[0] = True\n",
    "                currNode.frequency = None\n",
    "                currNode.end_word = False\n",
    "\n",
    "        if currNode.end_word == False:\n",
    "            if currNode.left == None and currNode.middle == None and currNode.right == None:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def add_ac_words(self, currNode: Node, compoundWord: str, ac_lst: list) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        Recursively traverse all the children nodes of currNode and create an instance of WordFrequency\n",
    "        using compoundWord and the frequency of currNode if its end_word is True.\n",
    "        @param currNode, compoundWord, ac_lst: compoundWord to keep track of the word to be added\n",
    "        ac_lst: the list to which an instance of WordFrequency is added\n",
    "        @return: a list (could be empty) of all the words with prefix 'word'\n",
    "        \"\"\"\n",
    "\n",
    "        # Base Case 1: currNode is None\n",
    "        if currNode == None:\n",
    "            return\n",
    "        # Recursive case:\n",
    "        else:\n",
    "            if currNode.end_word == True:\n",
    "                ac_lst.append(WordFrequency(compoundWord + currNode.letter, currNode.frequency))\n",
    "            self.add_ac_words(currNode.left, compoundWord, ac_lst)\n",
    "            self.add_ac_words(currNode.middle, compoundWord + currNode.letter, ac_lst)\n",
    "            self.add_ac_words(currNode.right, compoundWord, ac_lst)\n",
    "\n",
    "    def autocomplete(self, word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'word' as a prefix\n",
    "        @param word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'word'\n",
    "        \"\"\"\n",
    "        # a list of words to be autocompleted\n",
    "        ac_lst = []\n",
    "\n",
    "        # Find the prefix\n",
    "        currNode = self.search_from_node(self.root, word, 0)\n",
    "\n",
    "        # If the prefix does not exist\n",
    "        if not currNode:\n",
    "            return ac_lst\n",
    "        else:\n",
    "            # If the currNode's end_word is true\n",
    "            if currNode.end_word == True:\n",
    "                ac_lst.append(WordFrequency(word, currNode.frequency))\n",
    "            self.add_ac_words(currNode.middle, word, ac_lst)\n",
    "\n",
    "            # Python's built-in Timsort\n",
    "            ac_lst.sort(key=lambda wordFrequency: wordFrequency.frequency, reverse=True)\n",
    "\n",
    "        return ac_lst[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5e75f",
   "metadata": {},
   "source": [
    "# word_frequency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b646d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Class representing a word and its frequency\n",
    "class WordFrequency:\n",
    "    def __init__(self, word: str, frequency: int):\n",
    "        self.word = word\n",
    "        self.frequency = frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98fab07",
   "metadata": {},
   "source": [
    "# node.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5fb54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "# DON'T CHANGE THIS FILE\n",
    "# Class representing a node in the Ternary Search Tree\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, letter=None, frequency=None, end_word=False):\n",
    "        self.letter = letter            # letter stored at this node\n",
    "        self.frequency = frequency      # frequency of the word if this letter is the end of a word\n",
    "        self.end_word = end_word        # True if this letter is the end of a word\n",
    "        self.left = None    # pointing to the left child Node, which holds a letter < self.letter\n",
    "        self.middle = None  # pointing to the middle child Node\n",
    "        self.right = None   # pointing to the right child Node, which holds a letter > self.letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4c317",
   "metadata": {},
   "source": [
    "## General plan for the Empirical Analysis of Algorithm Time Efficiency\n",
    "\n",
    "1. Understand the experiment's purpose\n",
    "    - to find which data structure is most appropriate for given scenarios.\n",
    "2. Decide on the efficiency metric M to be measured and the measurement unit\n",
    "    - We're required to measure a time unit in seconds\n",
    "3. Decide on characteristics of the input sample\n",
    "    - (500, 1000, 2000, 4000, 8000, 16,000, 32,000, 64,000, 128,000)\n",
    "4. Prepare a program implementing the algorithm (or algorithms) for the experimentation.                         \n",
    "5. Generate a sample of inputs.\n",
    "6. Run the algorithm (or algorithms) on the sample's inputs and record the data observed.\n",
    "7. Anaylse the data obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55774bd",
   "metadata": {},
   "source": [
    "### Generate a sample of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0816edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]\n",
    "num_datasets_per_input = 10\n",
    "\n",
    "for i in range(len(input_sizes)):\n",
    "    with open(\"sampleData200k.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for j in range(num_datasets_per_input):\n",
    "            # Shuffle the list of lines in place\n",
    "            random.shuffle(lines)\n",
    "            \n",
    "            # Generate the main datasets to build dictionaries\n",
    "            with open(\"generatedData/\" + str(input_sizes[i]) + '-' + str(j) + '.txt', 'w') as f_dic: # 50-0.txt\n",
    "                f_dic.writelines(lines[0:input_sizes[i]])\n",
    "            # Generate the command lines to add words for Scenario 1\n",
    "            with open(\"generatedData/\" + str(input_sizes[i]) + '-' + str(j) + '-add.in', 'w' ) as f_add:\n",
    "                lst_words_to_add = lines[input_sizes[i] : input_sizes[i] + input_sizes[i] // 2]\n",
    "                command_lines = ['A ' + word for word in lst_words_to_add]\n",
    "                f_add.writelines(command_lines)\n",
    "            # Generate the command lines to delete words for Scenario 2\n",
    "            with open(\"generatedData/\" + str(input_sizes[i]) + '-' + str(j) + '-del.in', 'w') as f_del:\n",
    "                lst_words_in_dic = lines[0:input_sizes[i]]\n",
    "                random.shuffle(lst_words_in_dic)\n",
    "                lst_words_to_del = lst_words_in_dic[0: input_sizes[i] // 2]\n",
    "                command_lines = ['D ' +  word for word in lst_words_to_del]\n",
    "                f_del.writelines(command_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e830358",
   "metadata": {},
   "source": [
    "We have decided on the range of input sizes from 500 to 128000 at a two-fold increase since 500 is not too trivially small and the impact of increasing or decreasing input sizes according to a pattern makes it easy to analyze the time complexity. \n",
    "\n",
    "Since the empirical analysis outdoes the mathematical analysis in investigating the average-case efficiency, it makes more sense to generate multiple datasets per input size (e.g. ten 500-word datasets, ... , ten 128000-word datasets) and get the average time for an operation on the same set of datasets for each of the implementations. Therefore, a total of 90 datasets will be generated (9 input sizes * 10 datasets) as it helps lower the chances of measuring the running time of the worst-case scenario and producing biased results. \n",
    "\n",
    "For both Scenario 1 and 2, a dictionary of a given size (e.g. 500, ... , 128000) will be built per experiment and implementation. For Scenario 1, words, randomly selected, half as many as the number of words the dictionary carries will be added mostly in a bid to accurately reflect the presumed inefficiency of the add operation for the list-based dictionary. We will repeat the same process for Scenario 2 except the same proportion of words will be deleted from each dictionary (e.g. 250 randomly selected words deleted from the 500 words) for the same reason as above. For Scenario 3, Search / Auto-completion operations will be performed on dictionaries of 500, 8000, 128000 sizes as required by the assignment specification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a585a",
   "metadata": {},
   "source": [
    "### Measure the running time of Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fef7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.013572, 0.0126, 0.012773, 0.011611, 0.010859, 0.012999, 0.014054, 0.011565, 0.010169, 0.009363], [0.013980000000000001, 0.012994, 0.013149, 0.011988, 0.011182000000000001, 0.013407, 0.01444, 0.011931, 0.010464999999999999, 0.009661], [0.01725, 0.016996, 0.015844, 0.014937, 0.014599, 0.016548, 0.017266, 0.014489, 0.012735999999999999, 0.011961]]\n",
      "1 [[0.037151, 0.038343, 0.046405, 0.057251, 0.050442, 0.042477, 0.042587, 0.045037, 0.047437, 0.04663], [0.037823, 0.039303000000000005, 0.047068, 0.058056, 0.051246, 0.04331, 0.043376, 0.045774, 0.048167, 0.047338], [0.042881, 0.04480800000000001, 0.055358, 0.065328, 0.057639, 0.04866, 0.048621, 0.052415, 0.055441000000000004, 0.052633]]\n",
      "2 [[0.157782, 0.175655, 0.179388, 0.147394, 0.152132, 0.148041, 0.141289, 0.144394, 0.147971, 0.145457], [0.158935, 0.176849, 0.18057399999999998, 0.148571, 0.15328799999999998, 0.14927, 0.142396, 0.145758, 0.149082, 0.146597], [0.171229, 0.19495600000000002, 0.19178299999999998, 0.16093400000000002, 0.16896899999999998, 0.16053900000000002, 0.153037, 0.161161, 0.160097, 0.157363]]\n",
      "3 [[0.650736, 0.570554, 0.584988, 0.593412, 0.569755, 0.560902, 0.564609, 0.579823, 0.558761, 0.575137], [0.653107, 0.572837, 0.5873419999999999, 0.595647, 0.572246, 0.563137, 0.566813, 0.582217, 0.5612199999999999, 0.577531], [0.67732, 0.6026670000000001, 0.6116109999999999, 0.620087, 0.600033, 0.600346, 0.591574, 0.607549, 0.59612, 0.60278]]\n",
      "4 [[2.235106, 2.258801, 2.262616, 2.485472, 2.232724, 2.248242, 2.227075, 2.29057, 2.221004, 2.218741], [2.239699, 2.263311, 2.26951, 2.490133, 2.237518, 2.2528409999999996, 2.231633, 2.295227, 2.225429, 2.223312], [2.3128159999999998, 2.318521, 2.330208, 2.544016, 2.3022340000000003, 2.3099959999999995, 2.285636, 2.3706080000000003, 2.2827420000000003, 2.27874]]\n",
      "5 [[9.057261, 9.049195, 8.874896, 9.099652, 8.962634, 8.93037, 9.025411, 9.106441, 8.958068, 9.046091], [9.066382, 9.058592999999998, 8.884046, 9.108904, 8.971959, 8.939497, 9.034532, 9.115749000000001, 8.967051000000001, 9.055296], [9.182313, 9.189897999999998, 9.015813999999999, 9.240596, 9.189925, 9.067459999999999, 9.161849, 9.241150000000001, 9.091549, 9.178426]]\n",
      "6 [[36.839268, 35.922414, 36.26041, 36.677151, 39.996782, 38.102376, 37.506954, 37.565432, 37.066997, 40.709079], [36.863696, 35.94659600000001, 36.283947, 36.700427000000005, 40.02235700000001, 38.127712, 37.531512, 37.590655, 37.092104, 40.733757000000004], [37.133649999999996, 36.19360100000001, 36.529441999999996, 36.959526000000004, 40.29401800000001, 38.393953, 37.793552, 37.853508, 37.352614, 40.997298]]\n",
      "7 [[166.794695, 147.524458, 154.510644, 144.532095, 173.449732, 182.323608, 174.352577, 181.046756, 186.859545, 186.059792], [166.84347599999998, 147.574371, 154.56210900000002, 144.5817, 173.502032, 182.373691, 174.405115, 181.10519699999998, 186.913264, 186.111924], [167.37537299999997, 148.11796800000002, 155.12601200000003, 145.105874, 174.05954100000002, 182.960381, 174.984175, 181.73673099999996, 187.45549, 186.77028399999998]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "running_times = [[],[],[]] # list / hash / tst\n",
    "\n",
    "s_time = time.time_ns()\n",
    "# Iterate over 9 input sizes\n",
    "for i in range(len(input_sizes)):\n",
    "    running_times_per_input = [[],[],[]]\n",
    "    # Iterate over 10 datasets per input size\n",
    "    for j in range(num_datasets_per_input):\n",
    "        data_filename = \"generatedData/\" + str(input_sizes[i]) + '-' + str(j) + '.txt'\n",
    "        with open(data_filename, 'r') as f_dic: # 50-0.txt\n",
    "            agents = [ListDictionary(), HashTableDictionary(), TernarySearchTreeDictionary()]\n",
    "            words_frequencies_from_file = []\n",
    "            # Build words_frequncies_from_file\n",
    "            for line in f_dic:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                frequency = int(values[1])\n",
    "                word_frequency = WordFrequency(word, frequency)  # each line contains a word and its frequency\n",
    "                words_frequencies_from_file.append(word_frequency)\n",
    "            # Iterate over each data structure\n",
    "            for agent_idx, agent in enumerate(agents):\n",
    "                agent.build_dictionary(words_frequencies_from_file)\n",
    "                running_time = 0 \n",
    "\n",
    "                command_filename = \"generatedData/\" + str(input_sizes[i]) + '-' + str(j) + '-add.in'\n",
    "                with  open(command_filename, 'r') as f_add:\n",
    "                    start_time = time.time_ns()\n",
    "                    for line in f_add:\n",
    "                        command_values = line.split()\n",
    "                        command = command_values[0]\n",
    "                        word = command_values[1]\n",
    "                        frequency = int(command_values[2])\n",
    "                        word_frequency = WordFrequency(word, frequency)\n",
    "                        agent.add_word_frequency(word_frequency)\n",
    "                    end_time = time.time_ns()\n",
    "                    running_time = (end_time - start_time) / math.pow(10, 9)\n",
    "                    running_times_per_input[agent_idx].append(running_time)\n",
    "    print(i, running_times_per_input)\n",
    "e_time = time.time_ns()\n",
    "print(f'Time it took to run this block: {(e_time - s_time) / math.pow(10, 9)}')\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c685467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [], []]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[],[],[]]\n",
    "lst[0].append(1)\n",
    "lst[0].append(2)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680ddd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b3ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
