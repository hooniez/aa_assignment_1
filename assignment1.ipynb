{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae597d45",
   "metadata": {},
   "source": [
    "# base_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7882a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.word_frequency import WordFrequency\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Base class for dictionary implementations. DON'T CHANGE THIS FILE.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "class BaseDictionary:\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        @return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def autocomplete(self, prefix_word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'prefix_word' as a prefix\n",
    "        @param prefix_word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'prefix_word'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e3dff",
   "metadata": {},
   "source": [
    "# list_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e00c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.word_frequency import WordFrequency\n",
    "from dictionary.base_dictionary import BaseDictionary\n",
    "import time\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required TO BE IMPLEMENTED. List-based dictionary implementation.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class ListDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.data = [];\n",
    "\n",
    "    def partition(self, data, i, k, by):\n",
    "        midpoint = i + (k - i) // 2\n",
    "        pivot = getattr(data[midpoint], by)\n",
    "\n",
    "        done = False\n",
    "        l = i\n",
    "        h = k\n",
    "        while not done:\n",
    "            while getattr(data[l], by) < pivot:\n",
    "                l = l + 1\n",
    "            while pivot < getattr(data[h], by):\n",
    "                h = h - 1\n",
    "            if l >= h:\n",
    "                done = True\n",
    "            else:\n",
    "                temp = data[l]\n",
    "                data[l] = data[h]\n",
    "                data[h] = temp\n",
    "                l = l + 1\n",
    "                h = h - 1\n",
    "        return h\n",
    "\n",
    "    def quicksort(self, data, i, k, by: str):\n",
    "        j = 0\n",
    "        if i >= k:\n",
    "            return\n",
    "        j = self.partition(data, i, k, by)\n",
    "        self.quicksort(data, i, j, by)\n",
    "        self.quicksort(data, j + 1, k, by)\n",
    "        return\n",
    "\n",
    "    # merge sort algorithm sourced from Zybooks\n",
    "    def merge(self, data, i, j, k, by: str):\n",
    "        merged_size = k - i + 1  # Size of merged partition\n",
    "        merged_data = [0] * merged_size  # Dynamically allocates temporary array\n",
    "        # for merged numbers\n",
    "        merge_pos = 0  # Position to insert merged number\n",
    "        left_pos = i  # Initialize left partition position\n",
    "        right_pos = j + 1  # Initialize right partition position\n",
    "\n",
    "        # Add smallest element from left or right partition to merged numbers\n",
    "        while left_pos <= j and right_pos <= k:\n",
    "            if getattr(data[left_pos], by) <= getattr(data[right_pos], by):\n",
    "                merged_data[merge_pos] = data[left_pos]\n",
    "                left_pos += 1\n",
    "            else:\n",
    "                merged_data[merge_pos] = data[right_pos]\n",
    "                right_pos += 1\n",
    "            merge_pos = merge_pos + 1\n",
    "\n",
    "        # If left partition is not empty, add remaining elements to merged numbers\n",
    "        while left_pos <= j:\n",
    "            merged_data[merge_pos] = data[left_pos]\n",
    "            left_pos += 1\n",
    "            merge_pos += 1\n",
    "\n",
    "        # If right partition is not empty, add remaining elements to merged numbers\n",
    "        while right_pos <= k:\n",
    "            merged_data[merge_pos] = data[right_pos]\n",
    "            right_pos = right_pos + 1\n",
    "            merge_pos = merge_pos + 1\n",
    "\n",
    "        # Copy merge number back to numbers\n",
    "        for merge_pos in range(merged_size):\n",
    "            data[i + merge_pos] = merged_data[merge_pos]\n",
    "\n",
    "    def merge_sort(self, data, i, k, by: str):\n",
    "        j = 0\n",
    "\n",
    "        if i < k:\n",
    "            j = (i + k) // 2  # Find the midpoint in the partition\n",
    "\n",
    "            # Recursively sort left and right partitions\n",
    "            self.merge_sort(data, i, j, by)\n",
    "            self.merge_sort(data, j + 1, k, by)\n",
    "\n",
    "            # Merge left and right partition in sorted order\n",
    "            self.merge(data, i, j, k, by)\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"\"\n",
    "        for items in self.data:\n",
    "            str += f\"({items.word}, {items.frequency})\\n\"\n",
    "        return str\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        self.data = words_frequencies\n",
    "        # self.quicksort(self.data, 0, len(self.data) - 1, \"word\")\n",
    "        self.data.sort(key=lambda x: x.word)\n",
    "\n",
    "    def binSearch(self, word:str) -> (bool, int):\n",
    "        \"\"\"\n",
    "        binary search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: (True, the index of word_frequencies) OR (False, the index of word_frequencies to be inserted into)\n",
    "        \"\"\"\n",
    "        low, mid, high = 0, 0, len(self.data) - 1\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            if self.data[mid].word > word:\n",
    "                high = mid - 1\n",
    "            elif self.data[mid].word < word:\n",
    "                low = mid + 1\n",
    "            else:\n",
    "                return (True, mid)\n",
    "        if word >= self.data[mid].word:\n",
    "            return (False, mid + 1)\n",
    "        else:\n",
    "            return (False, mid)\n",
    "\n",
    "    def binSearchAC(self, prefix_word:str) -> int:\n",
    "        \"\"\"\n",
    "        binary search for a prefix\n",
    "        @param prefix: the prefix to be searched\n",
    "        @return: if found: the index of the first encountered word with the same prefix; if not: -1\n",
    "        \"\"\"\n",
    "        # The implementation is almost identical to binSearch except that prefix is compared to word upto its own length\n",
    "        low, mid, high = 0, 0, len(self.data) - 1\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            if self.data[mid].word[:len(prefix_word)] > prefix_word:\n",
    "                high = mid - 1\n",
    "            elif self.data[mid].word[:len(prefix_word)] < prefix_word:\n",
    "                low = mid + 1\n",
    "            else:\n",
    "                return mid\n",
    "        return -1\n",
    "\n",
    "    def getAutocompleteList(self, prefix_word: str, idx: int) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        add all the words sharing the same prefix_word to a list and return it unsorted\n",
    "        @param prefix_word: the prefix_word to be searched, idx: the starting index to search from in both directions (left and right)\n",
    "        @return: an unsorted list containing all the words sharing the same prefix_word\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        # Add the first word\n",
    "        res.append(self.data[idx])\n",
    "        left_idx = idx - 1\n",
    "        right_idx = idx + 1\n",
    "\n",
    "        if left_idx >= 0:\n",
    "            curr_left_word = self.data[left_idx].word[:len(prefix_word)]\n",
    "        # Add words to the left of the first word\n",
    "        while left_idx >= 0 and curr_left_word == prefix_word:\n",
    "            res.append(self.data[left_idx])\n",
    "            left_idx -= 1\n",
    "            curr_left_word = self.data[left_idx].word[:len(prefix_word)]\n",
    "\n",
    "        if right_idx <= len(self.data) - 1:\n",
    "            curr_right_word = self.data[right_idx].word[:len(prefix_word)]\n",
    "        # Add words to the right of the first word\n",
    "        while right_idx <= len(self.data) - 1 and curr_right_word == prefix_word:\n",
    "            res.append(self.data[right_idx])\n",
    "            right_idx += 1\n",
    "            curr_right_word = self.data[right_idx].word[:len(prefix_word)]\n",
    "\n",
    "        return res\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        # Employ binary search\n",
    "        isFound, foundIdx = self.binSearch(word)\n",
    "        if not isFound:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.data[foundIdx].frequency\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        # Employ binary search\n",
    "        if len(self.data) == 0:\n",
    "            self.data.append(word_frequency)\n",
    "        else:\n",
    "            word = word_frequency.word\n",
    "            isFound, foundIdx = self.binSearch(word)\n",
    "            actualLength = len(self.data)\n",
    "            if isFound:\n",
    "                return False\n",
    "            # If not found, add the word in self.data\n",
    "            else:\n",
    "                # Create space to shuffle elements to the right by 1\n",
    "                self.data.append(None)\n",
    "                for i in range(actualLength - 1, foundIdx - 1, -1):\n",
    "                    self.data[i + 1] = self.data[i]\n",
    "                self.data[foundIdx] = word_frequency\n",
    "                return True\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        isFound, foundIdx = self.binSearch(word)\n",
    "        if isFound:\n",
    "            for i in range(foundIdx, len(self.data) - 1):\n",
    "                self.data[i] = self.data[i + 1]\n",
    "            # In all cases, the last element will be deleted if the word is found\n",
    "            del self.data[-1]\n",
    "            return True\n",
    "        # If found, delete the word in self.data\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    def autocomplete(self, prefix_word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'prefix_word' as a prefix\n",
    "        @param prefix_word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'prefix_word'\n",
    "        \"\"\"\n",
    "        # As soon as prefix_word matches with any word, scan all the words to its left and right and put them in a new list\n",
    "        # Iterate them only once to find the 3 most-frequent words\n",
    "        idx = self.binSearchAC(prefix_word)\n",
    "        if idx == -1:\n",
    "            return []\n",
    "        else:\n",
    "            lst = self.getAutocompleteList(prefix_word, idx)\n",
    "            lst.sort(key=lambda x: x.frequency, reverse=True)\n",
    "            return lst[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c970e52",
   "metadata": {},
   "source": [
    "# hashtable_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde39dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.base_dictionary import BaseDictionary\n",
    "from dictionary.word_frequency import WordFrequency\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required TO BE IMPLEMENTED. Hash-table-based dictionary.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class HashTableDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        self.data = {entry.word: entry.frequency for entry in words_frequencies}\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        return self.data.get(word, 0)\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        freq = self.search(word_frequency.word)\n",
    "        if freq > 0:\n",
    "            return False\n",
    "        else:\n",
    "            self.data[word_frequency.word] = word_frequency.frequency\n",
    "            return True\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        freq = self.search(word)\n",
    "        if freq > 0:\n",
    "            del self.data[word]\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def autocomplete(self, word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'word' as a prefix\n",
    "        @param word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'word'\n",
    "        \"\"\"\n",
    "        # Find the keys that start with a given prefix\n",
    "        autoDic = {key: freq for key, freq in self.data.items() if key.startswith(word)}\n",
    "        # Use Python's built-in sorting algorithm to sort autoDic by frequency and return the last three elements in descending order.\n",
    "        return [WordFrequency(key, freq) for key, freq in sorted(autoDic.items(), key=lambda item: item[1])][-3:][::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17642527",
   "metadata": {},
   "source": [
    "# tearnarysearchtree_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be71dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary.base_dictionary import BaseDictionary\n",
    "from dictionary.word_frequency import WordFrequency\n",
    "from dictionary.node import Node\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# This class is required to be implemented. Ternary Search Tree implementation.\n",
    "#\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class TernarySearchTreeDictionary(BaseDictionary):\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def build_dictionary(self, words_frequencies: [WordFrequency]):\n",
    "        \"\"\"\n",
    "        construct the data structure to store nodes\n",
    "        @param words_frequencies: list of (word, frequency) to be stored\n",
    "        \"\"\"\n",
    "        for idx, entry in enumerate(words_frequencies):\n",
    "            self.add_word_frequency(entry)\n",
    "\n",
    "    def search(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        search for a word\n",
    "        @param word: the word to be searched\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        endNode = self.search_from_node(self.root, word, 0)\n",
    "        if endNode == None:\n",
    "            return 0\n",
    "        elif endNode.end_word == False:\n",
    "            return 0\n",
    "        else:\n",
    "            return endNode.frequency\n",
    "\n",
    "\n",
    "\n",
    "    def search_from_node(self, currNode, word, currIdx):\n",
    "        \"\"\"\n",
    "        search for a word recursively\n",
    "        @param node, word, currIdx: node to start from, the word to be searched, currIdx to search curLetter at\n",
    "        @return: frequency > 0 if found and 0 if NOT found\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        if currNode == None or currNode.letter == None:\n",
    "            return None\n",
    "        if currNode.letter < currLetter:\n",
    "            return self.search_from_node(currNode.right, word, currIdx)\n",
    "        elif currNode.letter > currLetter:\n",
    "            return self.search_from_node(currNode.left, word, currIdx)\n",
    "        elif currIdx < len(word) - 1:\n",
    "            return self.search_from_node(currNode.middle, word, currIdx + 1)\n",
    "        else:\n",
    "            return currNode\n",
    "\n",
    "\n",
    "    def add_word_frequency(self, word_frequency: WordFrequency) -> bool:\n",
    "        \"\"\"\n",
    "        add a word and its frequency to the dictionary\n",
    "        @param word_frequency: (word, frequency) to be added\n",
    "        :return: True whether succeeded, False when word is already in the dictionary\n",
    "        \"\"\"\n",
    "        return self.add_word_from_root(self.root, word_frequency.word, word_frequency.frequency, 0)\n",
    "\n",
    "    def add_word_from_root(self, currNode, word, freq, currIdx) -> bool:\n",
    "        \"\"\"\n",
    "        add a word recursively\n",
    "        @param currNode, word, freq, currIdx: currNode initially self.root; currIdx is used for the base case.\n",
    "        :return: True if addition is successful, false if the word being added already exists.\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        # Base case on the last word\n",
    "        if currIdx == len(word) - 1:\n",
    "            if currNode.letter == None:\n",
    "                currNode.letter = currLetter\n",
    "                currNode.frequency = freq\n",
    "                currNode.end_word = True\n",
    "                return True\n",
    "            # If currNode is the same as curLetter\n",
    "            else:\n",
    "                if currNode.letter > currLetter:\n",
    "                    if currNode.left == None:\n",
    "                        currNode.left = Node()\n",
    "                    currNode = currNode.left\n",
    "                    return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "                elif currNode.letter < currLetter:\n",
    "                    if currNode.right == None:\n",
    "                        currNode.right = Node()\n",
    "                    currNode = currNode.right\n",
    "                    return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "                else:\n",
    "                    if currNode.end_word == True:\n",
    "                        return False\n",
    "                    else:\n",
    "                        currNode.frequency = freq\n",
    "                        currNode.end_word = True\n",
    "                        return True\n",
    "        # Recursive case\n",
    "        else:\n",
    "            # When no word is present\n",
    "            if currNode.letter == None:\n",
    "                currNode.letter = currLetter\n",
    "                currNode.middle = Node()\n",
    "                currNode = currNode.middle\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx + 1)\n",
    "            elif currNode.letter < currLetter:\n",
    "                if currNode.right == None:\n",
    "                    currNode.right = Node()\n",
    "                currNode = currNode.right\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "            elif currNode.letter > currLetter:\n",
    "                if currNode.left == None:\n",
    "                    currNode.left = Node()\n",
    "                currNode = currNode.left\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx)\n",
    "            else:\n",
    "                if currNode.middle == None:\n",
    "                    currNode.middle = Node()\n",
    "                currNode = currNode.middle\n",
    "                return self.add_word_from_root(currNode, word, freq, currIdx + 1)\n",
    "\n",
    "    def delete_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        delete a word from the dictionary\n",
    "        @param word: word to be deleted\n",
    "        @return: whether succeeded, e.g. return False when point not found\n",
    "        \"\"\"\n",
    "        # First, search for a word\n",
    "        # endNode = self.search_from_node(self.root, word, 0)\n",
    "        # if not endNode or endNode.end_word == False:\n",
    "        #     return False\n",
    "        # else:\n",
    "        #     endNode.frequency = None\n",
    "        #     endNode.end_word = False\n",
    "        #     if endNode.left == None and endNode.middle == None and endNode.right == None:\n",
    "        #         del endNode\n",
    "        # return True\n",
    "        deleteStatus = [False]\n",
    "        self.delete_from_node(self.root, word, 0, deleteStatus)\n",
    "        return deleteStatus[0]\n",
    "\n",
    "    def delete_from_node(self, currNode, word, currIdx, deleteStatus):\n",
    "        \"\"\"\n",
    "        delete a word recursively\n",
    "        @param prevNode, currNode, word, currIdx\n",
    "        @return: False if not found or end_word equals False, True if found and end_word equals True\n",
    "        \"\"\"\n",
    "        currLetter = word[currIdx]\n",
    "        if currNode == None or currNode.letter == None:\n",
    "            return False\n",
    "        if currNode.letter < currLetter:\n",
    "            if self.delete_from_node(currNode.right, word, currIdx, deleteStatus):\n",
    "                currNode.right = None\n",
    "            else:\n",
    "                return False\n",
    "        elif currNode.letter > currLetter:\n",
    "            if self.delete_from_node(currNode.left, word, currIdx, deleteStatus):\n",
    "                currNode.left = None\n",
    "            else:\n",
    "                return False\n",
    "        elif currIdx < len(word) - 1:\n",
    "            if self.delete_from_node(currNode.middle, word, currIdx + 1, deleteStatus):\n",
    "                currNode.middle = None\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if currNode.end_word:\n",
    "                deleteStatus[0] = True\n",
    "                currNode.frequency = None\n",
    "                currNode.end_word = False\n",
    "\n",
    "        if currNode.end_word == False:\n",
    "            if currNode.left == None and currNode.middle == None and currNode.right == None:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def add_ac_words(self, currNode: Node, compoundWord: str, ac_lst: list) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        Recursively traverse all the children nodes of currNode and create an instance of WordFrequency\n",
    "        using compoundWord and the frequency of currNode if its end_word is True.\n",
    "        @param currNode, compoundWord, ac_lst: compoundWord to keep track of the word to be added\n",
    "        ac_lst: the list to which an instance of WordFrequency is added\n",
    "        @return: a list (could be empty) of all the words with prefix 'word'\n",
    "        \"\"\"\n",
    "\n",
    "        # Base Case 1: currNode is None\n",
    "        if currNode == None:\n",
    "            return\n",
    "        # Recursive case:\n",
    "        else:\n",
    "            if currNode.end_word == True:\n",
    "                ac_lst.append(WordFrequency(compoundWord + currNode.letter, currNode.frequency))\n",
    "            self.add_ac_words(currNode.left, compoundWord, ac_lst)\n",
    "            self.add_ac_words(currNode.middle, compoundWord + currNode.letter, ac_lst)\n",
    "            self.add_ac_words(currNode.right, compoundWord, ac_lst)\n",
    "\n",
    "    def autocomplete(self, word: str) -> [WordFrequency]:\n",
    "        \"\"\"\n",
    "        return a list of 3 most-frequent words in the dictionary that have 'word' as a prefix\n",
    "        @param word: word to be autocompleted\n",
    "        @return: a list (could be empty) of (at most) 3 most-frequent words with prefix 'word'\n",
    "        \"\"\"\n",
    "        # a list of words to be autocompleted\n",
    "        ac_lst = []\n",
    "\n",
    "        # Find the prefix\n",
    "        currNode = self.search_from_node(self.root, word, 0)\n",
    "\n",
    "        # If the prefix does not exist\n",
    "        if not currNode:\n",
    "            return ac_lst\n",
    "        else:\n",
    "            # If the currNode's end_word is true\n",
    "            if currNode.end_word == True:\n",
    "                ac_lst.append(WordFrequency(word, currNode.frequency))\n",
    "            self.add_ac_words(currNode.middle, word, ac_lst)\n",
    "\n",
    "            # Python's built-in Timsort\n",
    "            ac_lst.sort(key=lambda wordFrequency: wordFrequency.frequency, reverse=True)\n",
    "\n",
    "        return ac_lst[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5e75f",
   "metadata": {},
   "source": [
    "# word_frequency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b646d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Class representing a word and its frequency\n",
    "class WordFrequency:\n",
    "    def __init__(self, word: str, frequency: int):\n",
    "        self.word = word\n",
    "        self.frequency = frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98fab07",
   "metadata": {},
   "source": [
    "# node.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fb54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# __author__ = 'Son Hoang Dau'\n",
    "# __copyright__ = 'Copyright 2022, RMIT University'\n",
    "# -------------------------------------------------\n",
    "\n",
    "# DON'T CHANGE THIS FILE\n",
    "# Class representing a node in the Ternary Search Tree\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, letter=None, frequency=None, end_word=False):\n",
    "        self.letter = letter            # letter stored at this node\n",
    "        self.frequency = frequency      # frequency of the word if this letter is the end of a word\n",
    "        self.end_word = end_word        # True if this letter is the end of a word\n",
    "        self.left = None    # pointing to the left child Node, which holds a letter < self.letter\n",
    "        self.middle = None  # pointing to the middle child Node\n",
    "        self.right = None   # pointing to the right child Node, which holds a letter > self.letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4c317",
   "metadata": {},
   "source": [
    "## General plan for the Empirical Analysis of Algorithm Time Efficiency\n",
    "\n",
    "1. Understand the experiment's purpose\n",
    "    - to find which data structure is most appropriate for given scenarios.\n",
    "2. Decide on the efficiency metric M to be measured and the measurement unit\n",
    "    - We're required to measure a time unit in seconds\n",
    "3. Decide on characteristics of the input sample\n",
    "    - (500, 1000, 2000, 4000, 8000, 16,000, 32,000, 64,000, 128,000)\n",
    "4. Prepare a program implementing the algorithm (or algorithms) for the experimentation.                         \n",
    "5. Generate a sample of inputs.\n",
    "6. Run the algorithm (or algorithms) on the sample's inputs and record the data observed.\n",
    "7. Anaylse the data obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55774bd",
   "metadata": {},
   "source": [
    "### Generate a sample of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0816edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "input_sizes = [250, 500, 1000, 2000, 4000, 8000, 16000, 32000, 64000]\n",
    "size_range_start = 0\n",
    "size_range_end = input_sizes[-1]\n",
    "num_datasets = 3\n",
    "\n",
    "with open(\"sampleData200k.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(num_datasets):\n",
    "        dataset = lines[size_range_start:size_range_end] # 0:64,000, 64,000:128,000, 128,000:192,000\n",
    "        dataset2 = copy.deepcopy(dataset)\n",
    "        random.shuffle(dataset2)\n",
    "        size_range_start = size_range_end\n",
    "        size_range_end += input_sizes[-1]\n",
    "        # Generate the datasets for Scenario 1\n",
    "        with open(\"generatedData/dataset\" + str(i) +'.txt', 'w' ) as f:\n",
    "            f.writelines(dataset)\n",
    "        with open(\"generatedData/shuffled_dataset\" + str(i) +'.txt', 'w' ) as f:\n",
    "            f.writelines(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e830358",
   "metadata": {},
   "source": [
    "#### Parameter Setting\n",
    "##### The Sizes of the Datasets & the Number of the Datasets  & the Number of Operations Tested\n",
    "Since the empirical analysis outdoes the mathematical analysis in investigating the average-case efficiency, it makes more sense to generate multiple datasets and obtain the average time of operations for each of the implementations across the datasets. Testing with multiple datasets also helps lower the chances of measuring the running time of the worst-case scenario and producing biased results. For this reason, we have chosen to conduct our analysis using 3 datasets, each of which contains 64,000 words, as we can make the most out of the provided dataset \"sampleData200k.txt\" (utilizing almost all of the words). \n",
    "\n",
    "### Methods for Measuring the Running Times\n",
    "FIX IN-TEXT CITATION\n",
    "For Scenario 1 and 2, We have decided on the range of input sizes from 250 to 64,000 at a two-fold increase/decrease since 250 is not too trivially small and the impact of increasing or decreasing input sizes according to a pattern makes it easy to analyze the time complexity. For both scenarios, an accumulation of time will be recorded as the input size grows or shrinks. For example, in Scenario 1, the average time it takes to add 250 words will be captured and it will be added to the time it takes for the total number of words to reach the next input size, 500 and the resultant accumulated time will be captured as well. Once time is measured for all the input sizes in this manner, we will transform both input size and time measured into a logarithmic scale. We then plot the result and find out the slope of the line, which is the polynomial degree of the running time. (https://algorithmtutor.com/Analysis-of-Algorithm/Empirical-way-of-calculating-running-time/). For Scenario 2, the same datasets used in Scenario 1 will be used again. However, we need to delete words in random order different from the order in which 64,000 words are inserted and record the accumulating time at each input size, the number of words to delete. As in the previous scenario, we fit a regression line around the logarithmic-scaled data to find the time complexity of each implementation.\n",
    "For Scenario 3, ... \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd07be",
   "metadata": {},
   "source": [
    "### Measure the Running Times of Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e821a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Each row contains the running times for all the implementations on one dataset in the order to list, hashtable, and tst.\n",
    "running_times = np.empty((3,3,len(input_sizes)))\n",
    "\n",
    "# Iterate over 3 datasets\n",
    "for i in range(num_datasets):\n",
    "    agents = [ListDictionary(), HashTableDictionary(), TernarySearchTreeDictionary()]\n",
    "    for agent_idx, agent in enumerate(agents):\n",
    "        command_filename = \"generatedData/dataset\" + str(i) + '.txt'\n",
    "        counter = 0 \n",
    "        running_time = 0\n",
    "        with open(command_filename, 'r') as f: \n",
    "            for line_idx, line in enumerate(f):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                frequency = int(values[1])\n",
    "                word_frequency = WordFrequency(word, frequency)  # each line contains a word and its frequency\n",
    "                start_time = time.time_ns()\n",
    "                agent.add_word_frequency(word_frequency)\n",
    "                end_time = time.time_ns()\n",
    "                running_time += (end_time - start_time) / math.pow(10, 9)\n",
    "                if line_idx == input_sizes[counter] - 1:\n",
    "                    running_times[agent_idx, i, counter] = running_time\n",
    "                    counter += 1\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf558b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_list = np.sum(running_times[0], axis=0) / 3\n",
    "runtime_hash = np.sum(running_times[1], axis=0) / 3\n",
    "runtime_tst = np.sum(running_times[2], axis=0) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf48449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "list_slope, intercept, r_value, p_value, std_err = stats.linregress(np.log2(input_sizes), np.log2(runtime_list))\n",
    "hash_slope, intercept, r_value, p_value, std_err = stats.linregress(np.log2(input_sizes), np.log2(runtime_hash))\n",
    "tst_slope, intercept, r_value, p_value, std_err = stats.linregress(np.log2(input_sizes), np.log2(runtime_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0d60e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x=np.log2(input_sizes), y=np.log2(runtime_list), label=\"list\", ci=None)\n",
    "sns.regplot(x=np.log2(input_sizes), y=np.log2(runtime_hash), label=\"hashtable\", ci=None)\n",
    "sns.regplot(x=np.log2(input_sizes), y=np.log2(runtime_tst), label=\"tst\", ci=None)\n",
    "\n",
    "plt.text(np.log2(input_sizes[-3]) - 2, np.log2(runtime_list[-3]), 'Slope: ' + str(round(list_slope,3)), c='#1f77b4')\n",
    "plt.text(np.log2(input_sizes[-3]), np.log2(runtime_hash[-3]) - 2, 'Slope: ' + str(round(hash_slope,3)), c='#ff7f0e')\n",
    "plt.text(np.log2(input_sizes[-3]), np.log2(runtime_tst[-3]) - 1.5, 'Slope: ' + str(round(tst_slope,3)), c='#2ca02c')\n",
    "plt.legend()\n",
    "plt.ylabel('Time (Logarithmic scale)')\n",
    "plt.xlabel('Inputs (Logarithmic scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a175a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd583cc",
   "metadata": {},
   "source": [
    "### Measure the Running Times of Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Each row contains the running times for all the implementations on one dataset in the order to list, hashtable, and tst.\n",
    "running_times = np.empty((3,3,len(input_sizes)))\n",
    "\n",
    "# Iterate over 3 datasets\n",
    "for i in range(num_datasets):\n",
    "    agents = [ListDictionary(), HashTableDictionary(), TernarySearchTreeDictionary()]\n",
    "    for agent_idx, agent in enumerate(agents):\n",
    "        command_filename = \"generatedData/dataset\" + str(i) + '.txt'\n",
    "        counter = 0 \n",
    "        running_time = 0\n",
    "        with open(command_filename, 'r') as f: \n",
    "            for line_idx, line in enumerate(f):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                frequency = int(values[1])\n",
    "                word_frequency = WordFrequency(word, frequency)  # each line contains a word and its frequency\n",
    "                start_time = time.time_ns()\n",
    "                agent.add_word_frequency(word_frequency)\n",
    "                end_time = time.time_ns()\n",
    "                running_time += (end_time - start_time) / math.pow(10, 9)\n",
    "                if line_idx == input_sizes[counter] - 1:\n",
    "                    running_times[agent_idx, i, counter] = running_time\n",
    "                    counter += 1\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05779efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
